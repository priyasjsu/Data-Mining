"""TrainedModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12Z3zjzlKXbBtvE1vHhQDvMdFcvx_5j5w
"""

import pandas as pd

# restaurant_data = pd.read_csv('/Users/iqrabismi/Desktop/DMProject/restaurant_data.csv')


from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.pipeline import Pipeline
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.metrics.pairwise import cosine_similarity

import warnings

# Ignore all warnings
warnings.filterwarnings("ignore")

# !pip install scikit-surprise


from surprise import Reader, Dataset
from sklearn.model_selection import train_test_split
from surprise import SVD
from surprise import Dataset
from surprise.model_selection import train_test_split

import pandas as pd
import numpy as np
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
import re
import ssl
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction.text import TfidfVectorizer

import ssl

ssl._create_default_https_context = ssl._create_unverified_context

import nltk

nltk.data.path.append('/Users/iqrabismi/Desktop/DMProject/nltk_data')

nltk.download('vader_lexicon')

nltk.download('stopwords')

nltk.download('punkt')

import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel

# pip install fuzzywuzzy

from fuzzywuzzy import fuzz

import pickle


class RestaurantHybridModel:

    def __init__(self, path):
        self.restaurant_data = pd.read_csv(path)

    def prepareData(self):

        # Add your data filtering and preprocessing steps here

        # lower casing and changing the datatype
        self.restaurant_data['review'] = self.restaurant_data['review'].astype(str)

        self.restaurant_data['review'] = self.restaurant_data['review'].str.lower()

        # Remove punctuation, special characters, and extra whitespaces
        self.restaurant_data['review'] = self.restaurant_data['review'].str.replace('[^\w\s]', '')
        self.restaurant_data['review'] = self.restaurant_data['review'].str.replace('\s+', ' ')

    def preprocessText(self):
        # Combine reviews for each restaurant
        restaurant_reviews = self.restaurant_data.groupby('restaurant_id')['review'].apply(' '.join).reset_index()
        # Add your text preprocessing steps here
        preprocessed_reviews = self.remove_stop_words(restaurant_reviews['review'])
        return preprocessed_reviews, restaurant_reviews

    def remove_stop_words(self, documents):
        stop_words = set(stopwords.words('english'))
        filtered_docs = []
        for doc in documents:
            words = word_tokenize(doc)
            filtered_words = [word for word in words if word.lower() not in stop_words]
            filtered_docs.append(' '.join(filtered_words))
        return filtered_docs

    def getSentimentScore(self, restaurant_reviews):
        sid = SentimentIntensityAnalyzer()
        pos = []
        neg = []
        for text in restaurant_reviews['review']:
            score = sid.polarity_scores(text)
            pos.append(score['pos'])
            neg.append(score['neg'])
        restaurant_reviews['PositiveScore'] = pos
        restaurant_reviews['NegativeScore'] = neg
        return restaurant_reviews

    def createDictForLDA(self, preprocessed_reviews):
        # Tokenize the reviews
        tokenized_reviews = [simple_preprocess(review) for review in preprocessed_reviews]
        # Create Dictionary
        id2word = corpora.Dictionary(tokenized_reviews)
        # Term Document Frequency
        corpus = [id2word.doc2bow(text) for text in tokenized_reviews]
        return id2word, corpus

    def performLDA(self, id2word, corpus):
        lda_model = gensim.models.LdaMulticore(corpus=corpus, id2word=id2word,
                                               num_topics=10, random_state=100, passes=5)
        return lda_model

    def getPredictedTopic(self, lda_model, corpus):
        topic_pred = []
        for doc in corpus:
            temp = lda_model[doc]
            result = sorted(temp, key=lambda x: (x[1]), reverse=True)
            topic_pred.append(result[0][0])
        return topic_pred

    def selectTopRestaurants(self, topic_pred, restaurant_reviews, lda_model):

        restaurant_reviews['PredictedTopic'] = pd.Series(topic_pred)

        weighted_scores = []

        for _, row in restaurant_reviews.iterrows():
            ps = row['PositiveScore']
            ns = row['NegativeScore']
            temp = lda_model.get_topic_terms(row['PredictedTopic'])
            lda_scores = [temp[0][1], temp[1][1], temp[2][1], temp[3][1], temp[4][1], temp[5][1], temp[6][1],
                          temp[7][1], temp[8][1], temp[9][1]]
            # Normalize the LDA scores to sum up to 1
            lda_scores /= sum(lda_scores)
            # Calculate the weighted score
            weighted_score = np.dot(lda_scores, [ps, ps, ps, ps, ps, ps, ps, ps, ps, ps]) - np.dot(lda_scores,
                                                                                                   [ns, ns, ns, ns, ns,
                                                                                                    ns, ns, ns, ns, ns])
            weighted_scores.append(weighted_score)

        restaurant_reviews['LDA_Sentiment_Score'] = weighted_scores

        # Count the number of reviews for each restaurant

        restaurant_counts = self.restaurant_data.groupby('restaurant_id')['review_count'].sum().reset_index()

        restaurant_counts.columns = ['restaurant_id', 'review_count']

        # Merge review count and lda score data
        merged_data = pd.merge(restaurant_counts, restaurant_reviews, on='restaurant_id')

        # Calculate weighted score by multiplying review count with lda score
        merged_data['weighted_score'] = merged_data['review_count'] * merged_data['LDA_Sentiment_Score']

        # Sort the restaurants based on the weighted score
        sorted_restaurants = merged_data.sort_values('weighted_score', ascending=False)

        # Select the top 50 restaurants
        top_restaurants = sorted_restaurants.head(50)['restaurant_id'].tolist()

        return top_restaurants

    def train_svd_model(self, filtered_restaurants):

        # Define a Reader object for Surprise
        reader = Reader(rating_scale=(1, 5))

        # Load the dataset for collaborative filtering
        data = Dataset.load_from_df(
            filtered_restaurants[['user_id', 'restaurant_id', 'rating']], reader)

        # Split the dataset into training and testing sets
        trainset, testset = train_test_split(data, test_size=0.2, random_state=42)

        # Train the SVD model
        svd = SVD()
        svd.fit(trainset)

        return svd

    def train(self, preferred_cuisine=None, preferred_city=None):

        "Welcome to Restaurant Recommendation System based on Hybrid Model"

        self.prepareData()

        # if preferred_cuisine is not None:
        # self.restaurant_data = self.restaurant_data[self.restaurant_data[preferred_cuisine] == 1]

        # if preferred_city is not None:
        #    self.restaurant_data = self.restaurant_data[self.restaurant_data['city'] == preferred_city]"""

        best_match = None
        best_match_score = 0

        if preferred_cuisine is not None:

            cuisines = ['Pizza', 'Sandwiches', 'Coffee & Tea', 'American (Traditional)',
                        'Breakfast & Brunch', 'Italian', 'American (New)', 'Burgers', 'Fast Food',
                        'Chinese', 'Bakeries', 'Seafood']

            for cuisine in cuisines:
                similarity_score = fuzz.token_set_ratio(preferred_cuisine, cuisine)
                if similarity_score > best_match_score:
                    best_match_score = similarity_score
                    best_match = cuisine

            self.restaurant_data = self.restaurant_data[self.restaurant_data[best_match] == 1]


        elif preferred_city is not None:

            for city in self.restaurant_data['city']:
                similarity_score = fuzz.token_set_ratio(preferred_cuisine, city)
                if similarity_score > best_match_score:
                    best_match_score = similarity_score
                    best_match = cuisine

            self.restaurant_data = self.restaurant_data[self.restaurant_data[best_match] == 1]

        else:
            self.restaurant_data = self.restaurant_data.sample(frac=0.08)

        preprocessed_reviews, restaurant_reviews = self.preprocessText()

        restaurant_reviews = self.getSentimentScore(restaurant_reviews)

        id2word, corpus = self.createDictForLDA(preprocessed_reviews)

        lda_model = self.performLDA(id2word, corpus)

        topic_pred = self.getPredictedTopic(lda_model, corpus)

        top_restaurants = self.selectTopRestaurants(topic_pred, restaurant_reviews, lda_model)

        # Filter the review data for top restaurants

        filtered_restaurants = self.restaurant_data[self.restaurant_data['restaurant_id'].isin(top_restaurants)]

        svd = self.train_svd_model(filtered_restaurants)

        print("Training Completed")

        return filtered_restaurants, svd


import sys

if __name__ == "__main__":
    # Get cuisine and city from command-line arguments
    if len(sys.argv) >= 3:
        cuisine = sys.argv[1]
        city = sys.argv[2]
    else:
        cuisine = None
        city = None

    recommendation_system = RestaurantHybridModel('/Users/iqrabismi/Desktop/DMProject/restaurant_data.csv')
    data, trained_models = recommendation_system.train(cuisine, city)

    print("saving the model and data")

    with open("/Users/iqrabismi/Desktop/DMProject/trained_model.pkl", "wb") as f:
        pickle.dump(trained_models, f)

    # Save the data and trained_models to separate files
    with open("/Users/iqrabismi/Desktop/DMProject/train_svd_data.pkl", "wb") as f:
        pickle.dump(data, f)

# recommendation_system = RestaurantHybridModel('/Users/iqrabismi/Desktop/DMProject/restaurant_data.csv')

# data, trained_models= recommendation_system.train("Italian",'Philadelphia')

# with open("/content/drive/MyDrive/trained_model.pkl", "wb") as f:
#  pickle.dump(trained_models, f)

# Save the data and trained_models to separate files
# with open("/content/drive/MyDrive/train_svd_data.pkl", "wb") as f:
#    pickle.dump(data, f)
