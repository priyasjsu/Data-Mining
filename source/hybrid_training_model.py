# -*- coding: utf-8 -*-
"""TrainedModel.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/12Z3zjzlKXbBtvE1vHhQDvMdFcvx_5j5w
"""

import pandas as pd

# restaurant_data = pd.read_csv('/Users/iqrabismi/Desktop/DMProject/restaurant_data.csv')


from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.pipeline import Pipeline
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.metrics.pairwise import cosine_similarity

import warnings

# Ignore all warnings
warnings.filterwarnings("ignore")

# !pip install scikit-surprise


from surprise import Reader, Dataset
from sklearn.model_selection import train_test_split
from surprise import SVD
from surprise import Dataset
from surprise.model_selection import train_test_split

import pandas as pd
import numpy as np
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
import re
import ssl
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction.text import TfidfVectorizer

import ssl

ssl._create_default_https_context = ssl._create_unverified_context

import nltk

# nltk.data.path.append('/Users/iqrabismi/Desktop/DMProject/nltk_data')

# nltk.download('vader_lexicon')

# nltk.download('stopwords')

# nltk.download('punkt')

from nltk.corpus import stopwords

import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel

# pip install fuzzywuzzy

from fuzzywuzzy import fuzz

import pickle
5

from surprise.model_selection import cross_validate

from surprise import accuracy

from surprise import SVDpp
from surprise import accuracy


class RestaurantHybridModel:

    def __init__(self, file_ids):
        """ This function is use merge chunk data from google drive into a single dataframe and pass it as restraurant_data"""
        import requests
        from io import StringIO

        dfs = []

        # Iterate over each file ID
        for file_id in file_ids:
            # Construct the download URL for the current file ID
            url = f'https://drive.google.com/uc?id={file_id}&export=download'

            # Retrieve the file content
            download = requests.get(url).text

            # Create a StringIO object
            csv_data = StringIO(download)

            # Read the CSV data into a DataFrame
            df1 = pd.read_csv(csv_data)

            # Append the DataFrame to the list
            dfs.append(df1)

        # Merge the DataFrames into a single DataFrame
        merged_df = pd.concat(dfs)
        self.restaurant_data = merged_df

    def prepareData(self):

        """ This function is use to pre-process data"""

        # Add your data filtering and preprocessing steps here

        # lower casing and changing the datatype
        self.restaurant_data['review'] = self.restaurant_data['review'].astype(str)

        self.restaurant_data['review'] = self.restaurant_data['review'].str.lower()

        # Remove punctuation, special characters, and extra whitespaces
        self.restaurant_data['review'] = self.restaurant_data['review'].str.replace('[^\w\s]', '')
        self.restaurant_data['review'] = self.restaurant_data['review'].str.replace('\s+', ' ')

    def preprocessText(self):
        """ This function is used to remove stop words by passing remove_stop_words function"""

        # Combine reviews for each restaurant
        restaurant_reviews = self.restaurant_data.groupby('restaurant_id')['review'].apply(' '.join).reset_index()

        # Add your text preprocessing steps here
        preprocessed_reviews = self.remove_stop_words(restaurant_reviews['review'])
        return preprocessed_reviews, restaurant_reviews

    def remove_stop_words(self, documents):
        """ Function to remove stop words"""

        stop_words = set(stopwords.words('english'))
        filtered_docs = []
        for doc in documents:
            words = word_tokenize(doc)
            filtered_words = [word for word in words if word.lower() not in stop_words]
            filtered_docs.append(' '.join(filtered_words))
        return filtered_docs

    def getSentimentScore(self, restaurant_reviews):
        """ Function to get sentiment score for reviews """
        sid = SentimentIntensityAnalyzer()
        pos = []
        neg = []
        for text in restaurant_reviews['review']:
            score = sid.polarity_scores(text)
            pos.append(score['pos'])
            neg.append(score['neg'])
        restaurant_reviews['PositiveScore'] = pos
        restaurant_reviews['NegativeScore'] = neg
        return restaurant_reviews

    def createDictForLDA(self, preprocessed_reviews):
        """ Function to create corpur using BOW for LDA model"""
        # Tokenize the reviews
        tokenized_reviews = [simple_preprocess(review) for review in preprocessed_reviews]
        # Create Dictionary
        id2word = corpora.Dictionary(tokenized_reviews)
        # Term Document Frequency
        corpus = [id2word.doc2bow(text) for text in tokenized_reviews]
        return id2word, corpus

    def performLDA(self, id2word, corpus):
        """ Training the LDA model"""
        lda_model = gensim.models.LdaMulticore(corpus=corpus, id2word=id2word,
                                               num_topics=10, random_state=100, passes=5)
        return lda_model

    def getPredictedTopic(self, lda_model, corpus):
        """ Function to predict topic using LDA model"""
        topic_pred = []
        for doc in corpus:
            temp = lda_model[doc]
            result = sorted(temp, key=lambda x: (x[1]), reverse=True)
            topic_pred.append(result[0][0])
        return topic_pred

    def selectTopRestaurants(self, topic_pred, restaurant_reviews, lda_model):
        """ Function to get top restaurant id using LDA topic weightage , sentiment score and total review count. This is content based filtering based on review count, review sentiment  score and lda topic weightage"""

        restaurant_reviews['PredictedTopic'] = pd.Series(topic_pred)

        weighted_scores = []

        for _, row in restaurant_reviews.iterrows():
            ps = row['PositiveScore']
            ns = row['NegativeScore']
            temp = lda_model.get_topic_terms(row['PredictedTopic'])
            lda_scores = [temp[0][1], temp[1][1], temp[2][1], temp[3][1], temp[4][1], temp[5][1], temp[6][1],
                          temp[7][1], temp[8][1], temp[9][1]]
            # Normalize the LDA scores to sum up to 1
            lda_scores /= sum(lda_scores)
            # Calculate the weighted score
            weighted_score = np.dot(lda_scores, [ps, ps, ps, ps, ps, ps, ps, ps, ps, ps]) - np.dot(lda_scores,
                                                                                                   [ns, ns, ns, ns, ns,
                                                                                                    ns, ns, ns, ns, ns])
            weighted_scores.append(weighted_score)

        restaurant_reviews['LDA_Sentiment_Score'] = weighted_scores

        # Count the number of reviews for each restaurant

        restaurant_counts = self.restaurant_data.groupby('restaurant_id')['review_count'].sum().reset_index()

        restaurant_counts.columns = ['restaurant_id', 'review_count']

        # Merge review count and lda score data
        merged_data = pd.merge(restaurant_counts, restaurant_reviews, on='restaurant_id')

        # Calculate weighted score by multiplying review count with lda score
        merged_data['weighted_score'] = merged_data['review_count'] * merged_data['LDA_Sentiment_Score']

        # Sort the restaurants based on the weighted score
        sorted_restaurants = merged_data.sort_values('weighted_score', ascending=False)

        # Select the top 50 restaurants
        top_restaurants = sorted_restaurants.head(50)['restaurant_id'].tolist()

        return top_restaurants

    def train_svd_model(self, filtered_restaurants):
        """ Training the svd++ model with 100 epochs. This is User-Item Collaborative Filtering"""

        # Define a Reader object for Surprise
        reader = Reader(rating_scale=(1, 5))

        # Load the dataset for collaborative filtering
        data = Dataset.load_from_df(
            filtered_restaurants[['user_id', 'restaurant_id', 'rating']], reader)

        # Split the dataset into training and testing sets
        trainset, testset = train_test_split(data, test_size=0.2, random_state=42)

        # Train the SVD model

        svd = SVDpp()

        # Increase the number of iterations
        svd.n_epochs = 100

        # Adjust the number of latent factors
        svd.n_factors = 100

        # Increase the regularization strength
        svd.reg_all = 0.25

        # Fit the model on the training set
        svd.fit(trainset)

        # Make predictions on the test set
        predictions = svd.test(trainset.build_testset())

        # Evaluate the model's accuracy
        accuracyy = accuracy.rmse(predictions)

        return svd, accuracyy

    def train(self, preferred_cuisine=None, preferred_city=None):
        """ Function to create pipeline for recommendation system"""

        print("Welcome to Restaurant Recommendation System based on Hybrid Model")

        self.prepareData()

        best_match = None  # using fuzzy words to get the best match"""
        best_match_score = 0

        if preferred_cuisine is not None:

            cuisines = ['Pizza', 'Sandwiches', 'Coffee & Tea', 'American (Traditional)',
                        'Breakfast & Brunch', 'Italian', 'American (New)', 'Burgers', 'Fast Food',
                        'Chinese', 'Bakeries', 'Seafood']

            for cuisine in cuisines:
                similarity_score = fuzz.token_set_ratio(preferred_cuisine, cuisine)
                if similarity_score > best_match_score:
                    best_match_score = similarity_score
                    best_match = cuisine

            self.restaurant_data = self.restaurant_data[self.restaurant_data[best_match] == 1]

        if preferred_city is not None:

            for city in self.restaurant_data['city']:
                similarity_score = fuzz.token_set_ratio(preferred_cuisine, city)
                if similarity_score > best_match_score:
                    best_match_score = similarity_score
                    best_match = cuisine

            self.restaurant_data = self.restaurant_data[self.restaurant_data[best_match] == 1]

        preprocessed_reviews, restaurant_reviews = self.preprocessText()  # preprocess

        restaurant_reviews = self.getSentimentScore(restaurant_reviews)  # sentiment score

        id2word, corpus = self.createDictForLDA(preprocessed_reviews)  # corpur for lda

        lda_model = self.performLDA(id2word, corpus)  # lda model training

        topic_pred = self.getPredictedTopic(lda_model, corpus)  # predicting topic using lda

        top_restaurants = self.selectTopRestaurants(topic_pred, restaurant_reviews, lda_model)  # top restaurant id

        # Filter the review data for top restaurants

        filtered_restaurants = self.restaurant_data[self.restaurant_data['restaurant_id'].isin(top_restaurants)]

        svd, accuracyy = self.train_svd_model(filtered_restaurants)  # training the svd model

        print("Training Completed")

        print("RMSE of the Model is ", accuracyy)

        import random
        # randomly selecting a user id. for real scenario we can specify user id or username for a specific user.

        user_id = random.choice(self.restaurant_data['user_id'].unique())

        user_predictions = []
        for _, row in filtered_restaurants.iterrows():
            prediction = svd.predict(user_id, row['restaurant_id'])
            user_predictions.append((prediction.est, prediction.iid))

        recommendations_cf = pd.DataFrame(user_predictions,
                                          columns=['rating', 'restaurant_id'])  # dataframe for orginal data

        recommendations_cf = recommendations_cf.merge(filtered_restaurants,
                                                      on='restaurant_id')  # dataframe for predictions and meging it

        recommendations_cf = recommendations_cf.sort_values('rating_x', ascending=False)  # sorting

        print(f'Top 5 Recommended Restaurants for User {user_id}')

        print("---------------------")

        count = 1

        for i in recommendations_cf.name.unique():

                print(f'{count}. ', i)

                print("\n")
                count += 1
                if count == 6:
                    break




import sys

if __name__ == "__main__":
    # Get cuisine and city from command-line arguments
    if len(sys.argv) >= 3:
        cuisine = sys.argv[1]
        city = sys.argv[2]

    elif len(sys.argv) == 2:
        cuisine = sys.argv[1]
        city = None


    else:
        cuisine = "Italian"
        city = "Philadelphia"

    recommendation_system = RestaurantHybridModel(
        ['1RWUScIcfDfj7artDU1xfzR3biAzLl6pe', '178FeiC8Rrt0qzqlmYSYDoqaVIPNNmjtZ', '1CVGRr9EzCy-jAD82SJMrjrSTAk7lXOak',
         '1oDNBSn90Mf2ao74vejvmITLZ7y5OrveV', '1RoZqZCZ9h4G-XL89Zz2qZhZceYLj1QWe', '10ndnl89HiMXY8ntYqCJPdsAp8IYMFRr6',
         '1XPPd7bFY-PM5DzvsHsR3hxINeg9gELTq', '1D_5Yg6K0SDRRopHEVyEkV7BW15AJRuZX', '1Wz8irmxT3869APcOO21XuJR4cDSQkFJY',
         '1Q2jr2BJDKxmLZ-mr2utF09QgtOh79BrB', '1GTibrww-DO8uHGvTLzMUXdSDqe8qSRSI'])

    recommendation_system.train(cuisine, city)